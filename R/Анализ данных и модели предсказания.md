#### Связи в данных
Взаимосвязь между данными нужно определять исходя из типа данных:
- Количественные данные
Для определения взаимосвязи между двумя переменными используется коэф. корреляции
`R = cor(данные_1, данные_2)` 
Коэф. корреляции это значение от -1 до 1. Если коэф. корреляции равен 0 или очень близок к нему, то считается, что связи нет. Соответственно коэф. корреляции со знаком минус нам говорит о том, что связь обратная и знак плюс связь прямая. 
Обычно различают следующие категории:
`|R| < 0.3` – связь слабая 
`0.3 ⩽ |R| ⩽ 0.7` – связь умеренная 
`|R| > 0.7` – связь сильная
Но очень часто не понятно насколько зависимы между собой переменные в генеральной совокупности. Для определения значимости применяют различные тесты, при определяя коэф. корреляции. Есть следующие способы вычисления коэф. корреляции:

**Коэффициент корреляции Пирсона R** - используется для выявления линейной связи между двумя показателями в количественной шкале.
`cor.test(данные_1, данные_2)`
При расчете коэф. корреляции также выдает p-value, которая говорит о значимости его. Соответственно если p-value меньше 5% уровня значимости, то нет оснований применить нулевую гипотезу (коэф. корреляции незначим, близок к нулю) её отвергаем и применяем альтернативную гипотезу (коэф. корреляции значим и отличен от нуля).
   *Особенности:* 
   - Коэффициент Пирсона предназначен только для линейной связи
   - Чувствителен к выбросам в данных

**Коэффициент корреляции Спирмена** - используется для выявления монотонной связи между двумя показателями в порядковой шкале.
`cor.test(данные_1, данные_2, method = 'sperman')`
Также как при расчете коэф. корреляции Пирсона, дополнительно выдает p-value.
   *Особенности:*
   - Коэффициент Спирмена предназначен как для линейной так и для не линейной связи
   - Не чувствителен к выбросам в данных

- Категориальные данные
Так как категориальные данные мы не можем сравнивать, анализировать также как количественные, то необходимо анализировать их частоты. Для этого можно создать таблицу частот (таблица сопряжения):
`table(данные_1, данные_2)` 
Но для анализа иногда данных по частотам недостаточно, имеет смысл произвести статистические тесты:

**Коэффициент критерия хиквадрат** - используется для выявления связи между категориальными данными.
`chisq.test(данные_1, данные_2)`
При расчеты выдает p-value, которое показывает есть ли связь между данными.
   *Особенности:*
   - критерий работает на выборке с частотами более 5 в ячейке.
Что бы бороться с этим ограничением можно укрупнять категории или использовать другой критерий.

**Точный тест Фишера** - используется для выявления связи между категориальными данными
`fisher.test(данные_1, данные_2, simulate.p.value = TRUE)`
`simulate.p.value = TRUE` - используется если данных много, может не хватить памяти. 
#### Модели предсказаний

##### Линейная регрессия
1. Простая линейная регрессия - модель для описания и предсказания значений. Прямая 
линия которая строиться таким образом, что бы минимизировать квадрат ошибок всех точек на графике.
![[квадрат ошибок.png|350]]
Красные линии это и есть квадрат ошибок, прямую линию нужно построить таким образом, чтобы эти красные линии были самыми минимальными.
*Общий вид модели:*
$$y = k * x + b$$
где `y` - зависимая переменная;
`x` - независимая переменная;
`k`  и `b` - коэффициенты (веса)
для создания модели используется следующая функция:
`lm(data = датафрейм, зависимая_переменная ~ независимая_переменная)`
для вывода описания модели используется функция:
`summary(модель)`
для предсказания модели на новых данных используется функция:
`predict(модель, newdata = data.frame(независимая_переменная = новые_данные))`
*Оценка качества модели:*
- R-квадрат
При выводе описания модели, выводиться показатель R-квадрат.
R-квадрат - это показатель показывающий прогностическую силу модели или уровень вариативности модели, чем больше этот показатель тем лучше. Обычно показатель больше 0.7 говорит о хорошей прогностической силе модели. Но и слишком высокой показатель, более 0.9 - 0.95, говорит о переобученной модели.
- Гетероскедастичность - остатки (ошибки) в модели зависимы от независимых 
переменных. При построении графика между остатками и независимыми переменными или предсказанными значениями видна зависимость. В модели должна отсутствовать гетерскедастичность.

2. Множественная линейная регрессия - модель для описания и предсказания значений. В 
отличии от простой линейной модели, используется более одного независимого показателя. Если используем два независимых показателя на графике буде плоскость в трех мерном пространстве.
*Общий вид модели:*
$$ y = \beta_0 + \beta_1 * x_1 + \beta_2 * x_2\ +\ ...\ +\ \beta_i * x_i$$
где `y` - зависимая переменная
`x` - независимая переменная
`β`- коэффициенты (веса)
для создания модели используется следующая функция:
`lm(data = датафрейм, зависимая_переменная ~ независимая_переменная_1 + ...)`
для вывода описания модели используется функция:
`summary(модель)`
для предсказания модели на новых данных используется функция:
`predict(модель, newdata = data.frame(независимая_переменная = новые_данные, ...))`
*Оценка качества модели:*
- R-квадрат
При выводе описания модели, выводиться показатель R-квадрат.
R-квадрат - это показатель показывающий прогностическую силу модели или уровень вариативности модели, чем больше этот показатель тем лучше. Обычно показатель больше 0.7 говорит о хорошей прогностической силе модели. Но и слишком высокой показатель, более 0.9 - 0.95, говорит о переобученной модели.
- Мультиколлинеарность - в модели присутствуют сильно связанные данные, сильно 
зависящие друг от друга (сильно большой коэф. корреляции). В модели не должно быть мультикллинеарности.
- Гетероскедастичность - остатки (ошибки) в модели зависимы от независимых 
переменных. При построении графика между остатками и независимыми переменными или предсказанными значениями видна зависимость. В модели должна отсутствовать гетерскедастичность.
- Влиятельные наблюдения - наблюдения сильно отличающиеся по нескольким 
показателям и сильно влияющие на модель. Также эти влиятельные наблюдения необходимо удалить из модели.
##### Логистическая регрессия
Используется для предсказания бинарных данных, строиться на графике кривая,  которая близко описывает данные.
*Общий вид модели:*
$$ y = \frac{1}{1+e^{-(\beta_0 + \beta_1x)}} $$
где `y` - зависимая переменная
`x` - независимая переменная
`β`- коэффициенты (веса)
данная модель предсказывает долю от 100%. Если нам нужно предсказать бинарное значение (0 или 1), соответственно значение больше 50% можно интерпретировать как 1 и меньше 50% как 0.
для создания модели используется следующая функция:
`glm(data = датафрейм, зависимая_переменная ~ независимая_переменная_1, family = 'binomial')`
`binomial` - задаем функции метод для бинарного предсказания
для вывода описания модели используется функция:
`summary(модель)`
для предсказания модели на новых данных используется функция:
`predict(модель, newdata = data.frame(независимая_переменная = новые_данные, ...))`
*Оценка качества модели:*
- Accuracy
Отношение попаданий предсказаний на общее число данных. Самая простая мера качества, но не самая результативная и информативная.
- Матрица ошибок
Вычисляется количество попаданий 0 и 1, и количество ложных попаданий 0 и 1. Составляется матрица ошибок:
![[матрица ошибок.jpg]]
**True Negative (TN)** - истина отрицательное, количество элементов предсказанного для отрицательного класса (то есть 0).
**True Positive (TP)** - истина положительное, количество элементов предсказанного для положительного класса (то есть 1).
**False Negative (FN)** - ложна отрицательное, количество элементов предсказанных как отрицательный класс, но на самом деле положительный (ошибка второго рода).
**False Positive (FP)** - ложна положительное, количество элементов предсказанных как положительных, но на самом деле отрицательный (ошибка первого рода).
На основание этой матрице можно подсчитать следующие показатели:
**Чувствительность** - способность модели предсказывать 1
$$ Sensitivity = \frac{TP}{TP + FN} $$
**Специфичность** - способность модели предсказывать 0
$$ Specificity = \frac{TN}{TN + FP} $$